% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cg.R
\name{conj_grad}
\alias{conj_grad}
\title{Conjugate Gradient Minimization}
\usage{
conj_grad(par, fn, gr, c1 = c2/2, c2 = 0.1, max_iter = 100, red = 1,
  max_alpha_mult = 10, abstol = NULL, reltol = sqrt(.Machine$double.eps),
  line_search = rasmussen(c1 = c1, c2 = c2), ortho_restart = FALSE,
  nu = 0.1, prplus = FALSE, eps = .Machine$double.eps, verbose = FALSE,
  debug = FALSE, ...)
}
\arguments{
\item{par}{Vector of initial numeric parameters.}

\item{fn}{Function to optimize. Should take a vector of the length of
\code{par} and return a scalar numeric value.}

\item{gr}{Gradient of \code{fn}. Should take a vector of the length of
\code{par} and return a vector of the gradients with respect to each
parameter.}

\item{c1}{Constant used in sufficient decrease condition. Should take a value
between 0 and 1.}

\item{c2}{Constant used in curvature condition. Should take a value between
\code{c1} and 1.}

\item{max_iter}{Maximum number of iterations to carry out the optimization.}

\item{red}{Scalar to determine initial step size guess.}

\item{max_alpha_mult}{Maximum scale factor to use when guessing the initial
step size for the next iteration.}

\item{abstol}{Absolute tolerance. If not \code{NULL}, then optimization will
stop early if the return value of \code{fn} falls below this value.}

\item{reltol}{Relative tolerance. If not \code{NULL}, then optimization will
stop early if the relative decrease in the value of \code{fn} on successive
iterations falls below this value.}

\item{line_search}{Line search function. Assign the result of calling
\code{\link{more_thuente}} or \code{\link{rasmussen}}.}

\item{ortho_restart}{If \code{TRUE}, then if successive conjugate gradient
directions are not sufficiently orthogonal, reset the search direction to
steepest descent.}

\item{nu}{If the dot product of the old and new conjugate gradient direction
(normalized with respect to inner product of the new direction) exceeds
this value, then the two directions are considered non-orthogonal and the
search direction is reset to steepest descent. Only used if
\code{ortho_restart} is \code{TRUE}.}

\item{prplus}{If \code{TRUE} then the 'PR+' variant of the Polak-Ribiere
update will be used: when the beta scale factor used to calculate the
new direction is negative, the search direction will be reset to
steepest descent.}

\item{eps}{Epsilon for avoiding numerical issues.}

\item{verbose}{If \code{TRUE} log information about the status of the
optimization at each iteration.}

\item{debug}{If \code{TRUE} logs \emph{lots} of information about the status
of the optimization.}

\item{...}{Other parameters to pass to the \code{fn} and \code{gr} functions.}
}
\value{
List containing:
\itemize{
 \item \code{par} Optimized parameters.
 \item \code{value} Return value of \code{fn} for the optimized parameters.
 \item \code{values} Vector of return values of \code{fn} at each iteration
   of the optimization.
 \item \code{iter} Number of iterations optimization took place over.
 \item \code{counts} Sublist of two values, giving the number of evaluations
   of \code{fn} and \code{gr}, respectively.
}
}
\description{
Optimization method.
Uses the formula of Polak and Ribiere. Generally considered a better choice
than the Fletcher-Reeves method.
}
\examples{
# The venerable Rosenbrock Banana function
rosenbrock_banana <- list(
fr = function(x) {
 x1 <- x[1]
 x2 <- x[2]
 100 * (x2 - x1 * x1) ^ 2 + (1 - x1) ^ 2
},
grr = function(x) {
 x1 <- x[1]
 x2 <- x[2]
c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
   200 *      (x2 - x1 * x1))
})

# Default is to use Rasmussen line search with c1 = 0.05 and c2 = 0.1
res <- conj_grad(par = c(-1.2, 1),
                 fn = rosenbrock_banana$fr, gr = rosenbrock_banana$grr)

# Turning down eps, abstol and reltol to compare with Matlab result at
# http://learning.eng.cam.ac.uk/carl/code/minimize/
res <- conj_grad(par = c(0, 0),
                 fn = rosenbrock_banana$fr, gr = rosenbrock_banana$grr,
                 eps = .Machine$double.xmin, reltol = .Machine$double.xmin,
                 abstol = .Machine$double.xmin)
res$par # c(1, 1)
res$value # 1.232595e-32
res$iter # 19 iterations
res$counts # c(79, 79) 79 fn and 79 gr evaluations

# Use More'-Thuente line search with typical CG Wolfe parameters mentioned
# in Nocedal and Wright's book on numerical optimization
# Yes, you do have to specify c1 and c2 in two separate places. Sorry.
res <- conj_grad(par = c(-1.2, 1),
                 fn = rosenbrock_banana$fr, gr = rosenbrock_banana$grr,
                 line_search = more_thuente(c1 = 1e-4, c2 = 0.1),
                 c1 = 1e-4, c2 = 0.1)
}

